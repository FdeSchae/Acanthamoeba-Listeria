{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Encounter rate image analysis</h3>\n",
    "\n",
    "This image-analysis pipeline was used to analyze the recorded videos from the encounter rate experiments. This script was developped within the framework of the following publication:\n",
    "\n",
    "<em>de Schaetzen & Fan et al. Random encounters and predator locomotion drive the predation of Listeria monocytogenes by Acanthamoeba castellanii.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Import the correct libraries\n",
    "from __future__ import division, unicode_literals, print_function  #for compatibility with Python 2 and 3\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import math \n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "import trackpy as tp\n",
    "import trackpy.predict\n",
    "from tifffile import imread\n",
    "from tifffile import imsave\n",
    "from skimage.feature import canny\n",
    "from skimage.filters import sobel\n",
    "from skimage.exposure import histogram\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.morphology import area_closing\n",
    "from skimage import measure\n",
    "from skimage.segmentation import random_walker as rw\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating a folder structure for file saving</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_path = r'F:\\Encounter rate experiment analysis\\OD0.01_06082020\\AMOEBA'# Set the default folder, the folder mentioned here only serves as an example\n",
    "# the following definition allows for easy folder creation:\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "\n",
    "## TIFF: in this folder the converted video files will be saved (the conversion happens in ImageJ/Fiji)\n",
    "TIFF_path = def_path + \"\\TIFF\"\n",
    "createFolder(TIFF_path)\n",
    "## SCALE: in this folder the rescaled video files will be saved\n",
    "SCALE_path = def_path + \"\\SCALE\"\n",
    "createFolder(SCALE_path)\n",
    "## DATA_FEAT : location in which feature .csv files will be saved\n",
    "DAFE_path = def_path + \"\\DATA_FEAT\"\n",
    "createFolder(DAFE_path)\n",
    "## DATA_TRAJ: location in which trajectory .csv files will be saved  \n",
    "DATR_path = def_path + \"\\DATA_TRAJ\"\n",
    "createFolder(DATR_path)\n",
    "## DATA_TRAJfilter: location in which trajectory .csv files will be saved  \n",
    "DATR_f_path = def_path + \"\\DATA_TRAJ_filter\"\n",
    "createFolder(DATR_f_path)\n",
    "## DATA_TRAJ_MSD: location in which MSD filtered trajectory .csv files will be saved  \n",
    "DATR_MSD_path = def_path + \"\\DATA_TRAJ_MSD\"\n",
    "createFolder(DATR_MSD_path)   \n",
    "## DATR_MSD_TC: location in which MSD files including translational component are saved\n",
    "DATR_MSD_TC_path = def_path + \"\\DATA_TRAJ_MSD_TC\"\n",
    "createFolder(DATR_MSD_TC_path) \n",
    "## DATA_TRAJ_MSD_filter: location in which MSD filtered trajectory .csv files will be saved  \n",
    "DATR_MSD_f_path = def_path + \"\\DATA_TRAJ_MSD_filter\"\n",
    "createFolder(DATR_MSD_f_path)        \n",
    "## FIG_TRAJ: location in which trajectory plots will be saved  \n",
    "FIGtraj_path = def_path + \"\\FIG_TRAJ\"\n",
    "createFolder(FIGtraj_path) \n",
    "## FIG_MSD_traj: location in which msd filtered trajectory plots will be saved  \n",
    "FIG_MSD_traj_path = def_path + \"\\FIG_TRAJ_MSD_filter\"\n",
    "createFolder(FIG_MSD_traj_path)\n",
    "## FIG_TRAJ: location in which trajectory plots will be saved  \n",
    "FIGmsd_path = def_path + \"\\FIG_MSD\"\n",
    "createFolder(FIGmsd_path)   \n",
    "## FIG_TRAJ: location in which trajectory plots will be saved  \n",
    "FIG_TC_path = def_path + \"\\FIG_TC\"\n",
    "createFolder(FIG_TC_path)     \n",
    "## CANNY_INTERACTION_path: folder in which videofiles containing the interaction masks will be saved\n",
    "CANNY_INTERACTION_path = def_path + \"\\CANNY_INTERACTION\"\n",
    "createFolder(CANNY_INTERACTION_path)\n",
    "## CANNY_INTERACTION_small_path: folder in which videofiles containing the small interaction masks will be saved\n",
    "CANNY_INTERACTION_small_path = def_path + \"\\CANNY_INTERACTION_small\"\n",
    "createFolder(CANNY_INTERACTION_small_path)\n",
    "## CANNY_AMOEBA: folder in which videofiles containing the amoeba masks will be saved\n",
    "CANNY_AMOEBA_path = def_path + \"\\CANNY_AMOEBA\"\n",
    "createFolder(CANNY_AMOEBA_path)\n",
    "## DATR_VEL_path: folder in which swimming speed .csv files per timepoint will be saved\n",
    "DATR_VEL_path = def_path + \"\\DATA_TRAJ_VEL\"\n",
    "createFolder(DATR_VEL_path)\n",
    "## OUTPUT: folder in which global result data (amoeba_size, swimming speed, density,...) for all timepoints in .csv format will be saved\n",
    "OUTPUT_path = def_path + \"\\OUTPUT\"\n",
    "createFolder(OUTPUT_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Rescaling video files to 8-bit with a custom conversion</h4>\n",
    "\n",
    "It is assumed that the video-files have been converted from 16-bit .ND2 to 16-bit .TIFF. This can be done using the batch convert tool within ImageJ/Fiji (Process > Batch > Convert). Make sure to place the converted files into the TIFF folder.\n",
    "\n",
    "- RAW video files were named with their timestap, but always with 2 digits (e.g. for the recording in minute 1 the file would have had the name 01.nd2). This naming protocol is used throughout the script to assign timelabels in different resulting tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_TIFF = os.listdir(TIFF_path)\n",
    "os.chdir(TIFF_path)\n",
    "i = 1 # step variable\n",
    "R_mid = 125 # the center 8-bit value around which all pixel values will be centered\n",
    "R_min = 0 # the minimum value of an 8-bit range\n",
    "R_max = 255 # the maximum value of an 8-bit range\n",
    "i = 1 # step variable\n",
    "for value in fl_TIFF: # for loop that will cycle through each video-file\n",
    "    os.chdir(TIFF_path) # set the working directory to the TIFF folder \n",
    "    frames = np.array(imread(value)) # import the video-file as a numpy array\n",
    "    frames = frames/2.5 # the division of all pixel values by a factor 2.5 drastically reduces noise levels in the resulting rescaled videos\n",
    "    median = np.median(frames) # calculate the median value of all pixels over all frames\n",
    "    shift = median - R_mid # calculate pixel shift value that centers pixel values around 125\n",
    "    frames = frames - shift # deduct the pixel shift value to all pixels in all frames\n",
    "    frames = np.where(frames < R_min, R_min, frames) # all pixel values lower than R_min will be set to R_min \n",
    "    frames = np.where(frames > R_max, R_max, frames) # all pixel values higher than R_max will be set to R_max\n",
    "    scale_frames = np.array(frames, dtype = np.uint8) # convert the rescaled frames from 32-bit to 8 bit \n",
    "    os.chdir(SCALE_path) # set the working directory to the SCALE folder \n",
    "    filename_scaled = value.replace('.tif','_scaled.tif') # create a new filename\n",
    "    imsave(filename_scaled, scale_frames, imagej = 'True') # save the rescaled frames\n",
    "    print(i, 'out of', len(fl_TIFF), ' .TIFF files have been rescaled')\n",
    "    i += 1;  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Canny edge detection with several morphological operators opencv ans scipy to calculate a mask of the amoeba</h4>\n",
    "\n",
    "The following part of the script will only work on the provided raw data. The resulting interaction mask extends approximately 4 micrometers from the edge of the recorded Acanthamoeba, which in turn is represented by the Ameoba  mask. The Amoeba mask allows for the removal of the Acanthamoeba during the TrackPy particle detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(SCALE_path) # set the working directory to the SCALE folder\n",
    "fl_SCALE = os.listdir(SCALE_path) # make a list of all filenames\n",
    "i = 1 # step variable\n",
    "for value in fl_SCALE: # for loop that will cycle through each video-file \n",
    "    os.chdir(SCALE_path) # set the working directory to the SCALE folder\n",
    "    frames = np.array(imread(value)) # import the video-file as a numpy array\n",
    "    stacksize = len(frames) # calculate the total amount of frames\n",
    "    fr = 0 # step variable\n",
    "    INTERACTION = [] # create an empty list\n",
    "    AMOEBA = [] # create an empty list\n",
    "    while fr <= stacksize - 1: # a while loop that will go through each frame (the -1 modifier is in place to correct for the startframe value of 0)\n",
    "        sub_frame = frames[fr,::] # select the respective frame to be processed\n",
    "        sub_frame = gaussian_filter(sub_frame, sigma = 1.5) # apply a Gaussian blur with a kernel size of 1.5 pixels\n",
    "        conv_kernel = np.array([[-1, -1, -1],[-1, 5, -1],[-1, -1, -1]]) # define a convolution kernel\n",
    "        conv_frame = ndi.convolve(sub_frame, conv_kernel, mode='reflect', cval=0.0) # apply the convolution kernel to the frame\n",
    "        canny_frame = cv2.Canny(conv_frame, 20, 200) # apply a wide canny edge detection \n",
    "        ## Dilate the detected edges\n",
    "        dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10)) # define the dilation kernel\n",
    "        dilate_frame = np.array(cv2.dilate(canny_frame, dilate_kernel, iterations = 1), dtype = np.uint8) # dilate the detected edges (this fattens the detected edges)\n",
    "        ## Fill gaps, this will create blobs one of which is the observed amoeba\n",
    "        close_frame = np.array(ndi.binary_fill_holes(dilate_frame), dtype = np.uint8) # fills the gaps in between the edges\n",
    "        close_frame[close_frame > 0] = 255 # convert all pixels with a value above 0 to 255\n",
    "        ## Apply the opening morphological operation to remove unwanted objects\n",
    "        opening_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(80,80)) # define the opening kernel\n",
    "        opening_frame = cv2.morphologyEx(close_frame, cv2.MORPH_OPEN, opening_kernel) # apply the opening morphological operation to remove unwanted objects\n",
    "        ## Dilate the remaining amoeba feature - this will be the interaction mask\n",
    "        dilation2_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5)) # define the dilation2 kernel\n",
    "        dilation2_frame = cv2.dilate(opening_frame, dilation2_kernel, iterations = 1) # dilate the remaining amoeba feature\n",
    "        INTERACTION.append(dilation2_frame) # collect all interaction mask frames into one list\n",
    "        ## Erode the remaining amoeba feature - this will be the Amoeba mask\n",
    "        erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(30, 30)) # define the erosion kernel\n",
    "        erosion_frame = cv2.erode(dilation2_frame, erosion_kernel, iterations = 1) # erode the the remaining amoeba feature\n",
    "        AMOEBA.append(erosion_frame) # collect all amoeba mask frames into one list \n",
    "        fr += 1 # increase the frame step variable with 1\n",
    "    INTERACTION = np.array(INTERACTION) # convert the list to a numpy array\n",
    "    filename_interaction = os.path.splitext(value[0:4])[0] + \"_interaction.tif\" # create a new filename\n",
    "    os.chdir(CANNY_INTERACTION_path) # set the working directory to the CANNY_INTERACTION folder  \n",
    "    imsave(filename_interaction, INTERACTION,  imagej = 'True') # save the interaction mask video file as an ImageJ compatible .tif file\n",
    "    AMOEBA= np.array(EROSION) # convert the list to a numpy array\n",
    "    filename_amoeba = os.path.splitext(value[0:4])[0] + \"_amoeba.tif\" # create a new filename\n",
    "    os.chdir(CANNY_AMOEBA_path) # set the working directory to the CANNY_AMOEBA folder  \n",
    "    imsave(filename_erosion, EROSION,  imagej = 'True') # save the amoeba mask video file as an ImageJ compatible .tif file\n",
    "    print(i, 'out of', len(fl_SCALE), '.tif files have been processed for an interaction and amoeba mask')\n",
    "    i += 1; # increase the step variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>An extra script block to calculate a 50% smaller interaction zone to test the sensitivity of the analysis</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(CANNY_AMOEBA_path) # set the working directory to the AMOEBA folder\n",
    "fl_AMOEBA = os.listdir(CANNY_AMOEBA_path) # creata a list of all filenames\n",
    "i = 1 # step variable\n",
    "for value in fl_AMOEBA: # for loop that will cycle through each video-file \n",
    "    os.chdir(CANNY_AMOEBA_path) # set the working directory to the AMOEBA folder\n",
    "    frames = np.array(imread(value)) # import the video-file as a numpy array\n",
    "    stacksize = len(frames) # calculate the total amount of frames\n",
    "    fr = 0 # the frame step variable\n",
    "    INTERACTION_small = [] # create an empty list\n",
    "    while fr <= stacksize - 1: # a while loop that will go through each frame (the -1 modifier is in place to correct for the startframe value of 0)\n",
    "        sub_frame = frames[fr,::] # select the respective frame to be processed\n",
    "        dilation_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(15, 15)) # define the dilation kernel\n",
    "        dilation_frame = cv2.dilate(sub_frame, dilation_kernel, iterations = 1) # dilate the amoeba mask\n",
    "        INTERACTION_small.append(dilation_frame) # collect all small interaction mask frames into one list \n",
    "        fr += 1 # increase the frame step variable\n",
    "    INTERACTION_small = np.array(INTERACTION_small) # convert the list to a numpy array\n",
    "    filename_interactionsmall = value.replace('_amoeba.tif', '_interaction_small.tif') # create a new filename\n",
    "    os.chdir(CANNY_INTERACTION_small_path) # set the working directory to the INTERACTION small folder\n",
    "    imsave(filename_interactionsmall, INTERACTION_small,  imagej = 'True') # save the small interaction mask file as an ImageJ compatible .tif file\n",
    "    print(i, 'out of', len(fl_SCALE), '.tif files have been processed for a small interaction mask')\n",
    "    i+=1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate the size and radius of the Amoeba based on the Amoeba mask</h4>\n",
    "\n",
    "Here we make the assumption that the amoeba has a circular shape with a constant radius. The reason for this assumption is that the encounter rate models we used to compare out excperimental data with, use a the geometries of a circle or half sphere with a constant radius\n",
    "\n",
    "- this script block was skipped for the CONTROL folder as there is no need to calculate theoretical encounter rates, since there was no amoeba present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(CANNY_AMOEBA_path) # set the working directory to the AMOEBA folder\n",
    "fl_AMOEBA = os.listdir(CANNY_AMOEBA_path) # create a list of all filenames \n",
    "pmc = 3.076923 # pixel per micrometer conversion factor (this was determined by the microscope and remained constant for all encounter rate experiments, since they were all performed on one microscope with constant paramters)\n",
    "pmc2 = pmc*pmc # square pixels per square micrometer conversion factor\n",
    "i = 1 # step variable\n",
    "amoeba_size = pd.DataFrame(columns = ['time','amoeba_label', 'area_amoeba', 'raduis_amoeba', 'area_interaction', 'radius_interaction', 'area_interaction_small', 'radius_interaction_small'])\n",
    "for value in fl_AMOEBA: # for loop that will cycle through each filename\n",
    "    timepoint = os.path.splitext(value[0:2])[0] # define the timpoint based on the filename\n",
    "    amoeba_label = os.path.splitext(value[3])[0] # define the amoeba labal based on the filename\n",
    "    ## calculate the area and radius of the AMOEBA mask\n",
    "    os.chdir(CANNY_AMOEBA_path) # set the working directory to the AMOEBA folder\n",
    "    frames_amoeba = np.array(imread(value)) # import the AMOEBA mask files as a numpy array\n",
    "    stacksize = len(frames_amoeba)  # calculate the total amount of frames\n",
    "    area_amoeba = (np.count_nonzero(frames_amoeba == 255)/stacksize)/pmc2 # calculate the average area of the observed amoeba, based on the AMOEBA mask\n",
    "    radius_amoeba = math.sqrt(area_amoeba/math.pi) # calculate the radius of the AMOEBA mask, assuming it is a circle\n",
    "    ## calculate the area and radius of the INTERACTION mask\n",
    "    filename_interaction = os.path.splitext(value[0:4])[0] + \"_interaction.tif\" # define the INTERACTION mask filename\n",
    "    os.chdir(CANNY_INTERACTION_path) # set the working directory to the INTERACTION folder\n",
    "    frames_interaction = np.array(imread(filename_interaction)) # import the INTERACTION mask file as a numpy array\n",
    "    area_interaction = (np.count_nonzero(frames_interaction == 255)/stacksize)/pmc2 # calculate the average area of the INTERACTION mask\n",
    "    radius_interaction = math.sqrt(area_interaction/math.pi) # calculate the radius of the INTERACTION mask, assuming it is a circle\n",
    "    ## calculate the area and radius of the small INTERACTION  mask\n",
    "    filename_interaction_small = os.path.splitext(value[0:4])[0] + \"_interaction_small.tif\" # define the interaction mask filename\n",
    "    os.chdir(CANNY_INTERACTION_small_path) # set the working directory to the INTERACTION_small folder\n",
    "    frames_interaction_small = np.array(imread(filename_interaction_small)) # import the small INTERACTION mask file as a numpy array\n",
    "    area_interaction_small = (np.count_nonzero(frames_interaction == 255)/stacksize)/pmc2 # calculate the average area of the small INTERACTION mask\n",
    "    radius_interaction_small = math.sqrt(area_interaction_small/math.pi) # calculate the radius of the small INTERACTION mask, assuming it is a circle\n",
    "    values = [(timepoint, amoeba_label, area_amoeba, radius_amoeba, area_interaction, radius_interaction, area_interaction_small, radius_interaction_small)] # create a list of all processed values\n",
    "    sub_amoeba_size = pd.DataFrame(values, columns = ['time','amoeba_label', 'area_amoeba', 'raduis_amoeba', 'area_interaction', 'radius_interaction', 'area_interaction_small', 'radius_interaction_small']) # transform the data into a DataFrame row\n",
    "    amoeba_size = amoeba_size.append(sub_amoeba_size, sort = False) # collect all rows into one DataFrame\n",
    "    print(i, 'out of', len(fl_AMOEBA), ' files have been processed for amoeba size and radius')\n",
    "    i += 1;\n",
    "os.chdir(OUTPUT_path) # set the working directory to the OUTPUT folder\n",
    "amoeba_size.to_csv('amoeba_size.csv') # save the amoeba_size DataFrame as a .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Particle recognition and trajectory formation</h4>\n",
    "\n",
    "For more information regarding the TrackPy, we refer to the in-depth information provided by the TrackPy team on their github: http://soft-matter.github.io/trackpy/v0.5.0/. Furthermore, prior to the next analyses steps, the TrackPy parameters should be determined using the ACLI_TrackPy_parameterization script which can be found in the following github repository: https://github.com/Renderfarm/Acanthamoeba-Listeria. Please ensure that you run the parametization scriptblocks that apply for the encounter rate experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(SCALE_path) # set the working directory to the SCALE folder\n",
    "FL_SCALE = os.listdir(SCALE_path) # create a list of all filenames\n",
    "i = 1 # step variable\n",
    "Dia = 11 # diameter: approximate diameter (in pixels) of the particles to be located\n",
    "MinMass = 600 # Minimum mass: the minimum integrated brightness/darkness (filter to remove spurious objects)\n",
    "MaxMass = 2000 # The maximum integrated brightness\n",
    "Nsize = 1.5 # Noise size: width of the gaussian blur filter to remove noise\n",
    "MaxDia = 21 # maximum diameter size (in pixels) of the particles to be located\n",
    "sep = 7 # minumum separation (in pixels) between two features\n",
    "Pmem = 17 # Particle memory: the maximum number of frames during which a feature can vanish, then reappear nearby, and be considered the same particle, this correlated with 1 sec\n",
    "TrackTresh = 17 # Minimum number of points (video frames) to survive, this correlated with 1 sec\n",
    "MaxT = 10 # Maximum amount of pixels a bacteria can move in between two consecutive frames\n",
    "for value in FL_SCALE:  # for loop that will cycle through each video-file\n",
    "    os.chdir(SCALE_path) # set the working directory to the SCALE folder\n",
    "    frames = np.array(imread(value)) # import the frames as a numpy array\n",
    "    stacksize = len(frames) # calculate the total amount of frames\n",
    "    ## Batch locate particles in each frame of the stack with tp.batch(array, diameter, maxsize, invert = True, noise_size, minmass, separation)\n",
    "    fe = tp.batch(frames,  diameter = Dia, maxsize = MaxDia, invert = True, noise_size = Nsize, minmass = MinMass, separation = sep);\n",
    "    fe.drop(fe[fe['mass'] >= MaxMass].index, inplace = True) # filter out particles which integrated Mass exceeds 2000\n",
    "    # filter out useless detected features in Amoeba area frame per frame \n",
    "    fr = 0 # frane step variable\n",
    "    mask = [] # create an empty list \n",
    "    os.chdir(CANNY_AMOEBA_path) # set the working directory to the AMOEBA folder\n",
    "    filename_amoeba = value.replace('_scaled.tif', '_amoeba.tif') # create the corresponding AMOEBA mask filename\n",
    "    frames_amoeba = np.array(imread(filename_amoeba)) # import the AMOEBA mask file as a numpy array\n",
    "    for fr in fe.frame.unique(): # for loop that cycles through each frame\n",
    "        frame_data = fe.loc[fe['frame'] == fe] # select feature data for given frame fr\n",
    "        frame_amoeba = frames_amoeba[fr,::] # select the corresponding AMOEBA mask frame fr\n",
    "        frame_index = list(frame_d[[\"y\", \"x\"]].round().astype(int).itertuples(index=False, name=None)) # get list of coordinates of features within frame (rounded upwards and transformed to integer for use in numpy)\n",
    "        frame_mask = np.array([frame_amoeba[b] for b in frame_index]) # creates an array with pixel values of either 0, i.e. outside, or 255 , i.e. within the amoeba area/mask\n",
    "        mask.extend(frame_mask) # collect all mask frames in one list\n",
    "    mask = np.array(mask) # transform list to numpy array\n",
    "    fe['mask'] = mask # add mask list as new column into feature dataset\n",
    "    fe.drop(fe[fe['mask'] == 255].index, inplace = True) # drop rows with mask value 255 (within amoeba area), this will remove features that were detected within the amoeba area\n",
    "    fe.drop(['mask'], axis = 1) # drop the mask collumn for future usage\n",
    "    os.chdir(DAFE_path) # set the working directory to the DATA_FEAT folder\n",
    "    filename_features = value.replace('_scale.tif', '_feat.csv') # create a new filename\n",
    "    pd.DataFrame(fe).to_csv(filename_features) # save the features DataFrame as a .csv file\n",
    "    ## predictive link for trajectory plotting\n",
    "    pred = trackpy.predict.NearestVelocityPredict() # activate the predictive tracking feature of TrackPy\n",
    "    tr = pred.link_df(f_b, search_range = MaxT, memory = Pmem) # start memory with 1, increase if disconnected paths for same cell\n",
    "    os.chdir(DATR_path) # set the working directory to the DATA_TRAJ folder\n",
    "    filename_traj = value.replace('_scale.tif', '_traj.csv') # create a new filename\n",
    "    pd.DataFrame(tr).to_csv(filename_traj)  # save the trajectories DataFrame as a .csv file\n",
    "    ## filter out trajectories that are less than 17 frames long, this is approximately 1 second \n",
    "    trf = tp.filter_stubs(tr, TrackTresh) # filter out trajectories based on TrackTresh\n",
    "    filename _trajfilter = value.replace('_scale.tif', '_trajfilter.csv')\n",
    "    os.chdir(DATR_f_path) # set the working directory to the DATA_TRAJ_filter folder\n",
    "    pd.DataFrame(trf).to_csv(filename _trajfilter) # save the filtered trajectories DataFrame as a .csv file\n",
    "    ## Plot of trajectories colored by particle\n",
    "    plt.figure(figsize = (5,5), dpi = 300) # create an empty plot\n",
    "    ax1 = plt.subplot(1,1,1) # add axes to the plot\n",
    "    plt.imshow(frames[int(stacksize/2)], cmap = 'Greys_r' , vmin = 0, vmax = 255) # add the middle frame as a background image\n",
    "    tp.plot_traj(trf, ax=ax1,colorby = 'particle') # plot the trajectories\n",
    "    plotname_traj = value.replace() + '_traj.tif' # create a plotname\n",
    "    os.chdir(FIGtraj_path)\n",
    "    plt.savefig(plotname, format = 'tif', dpi = 1200)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Mean squared displacement (MSD) + translational component calculations</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DATR_f_path) # set the working directory to the DATA_TRAJ_filter folder\n",
    "fl_DATA = os.listdir(DATR_f_path)# create a list of all filenames\n",
    "te = 120 # time of recording in seconds, all videos recorded in the enounter rate experiments were 120 seconds in length\n",
    "l_t = 17 # maximum lag time in frames (this corresponds more or less to 1 second)\n",
    "i = 1 # step variable\n",
    "pmc = 3.076923 # pixels per micrometer conversion factor\n",
    "d_Dtot = pd.DataFrame() # create an ampty DataFrame\n",
    "for value in fl_DATA: # for loop that will cycle through each .csv-file\n",
    "    os.chdir(DATR_f_path)# set the working directory to the DATA_TRAJ_filter folder\n",
    "    trf = pd.read_csv(value) # import the .csv file as a DataFrame\n",
    "    fr = trf[\"frame\"].max() + 1 # calculate the total amount of frames for this timepoint, due to the nature of recording longer timelapses the total amount of frames in betwene timepoints could differ slightly\n",
    "    fr_ps = round(fr/te) # calculate the fps for this given timepoint\n",
    "    msd = tp.imsd(trf, 1/sf, fps = fr_ps, max_lagtime = l_t) # calculate the MSD of each trajectory\n",
    "    os.chdir(DATR_MSD_path) # set the working directory to the DATR_MSD filter folder\n",
    "    filename_msd = value.replace('.csv', '_msd.csv') # create a new filename\n",
    "    pd.DataFrame(msd).to_csv(filename_msd) # save the MSD DataFrame as a .csv file\n",
    "    ## plot figure off all MSD's\n",
    "    plt.figure(figsize = (5,5)) # create an empty plot\n",
    "    ax_msd = plt.subplot(1,1,1) # add axes to the plot\n",
    "    ax_msd.plot(msd.index, msd, 'k-', alpha=0.1) # plot the MSD's\n",
    "    ax_msd.set(ylabel=r'$\\langle \\Delta r^2 \\rangle$ [$\\mu$m$^2$]', xlabel='lag time $t$') # label the axes\n",
    "    ax_msd.set_xscale('log') # rescale the x-axis to logarithmic\n",
    "    ax_msd.set_yscale('log') # rescale the y-axis to logarithmic\n",
    "    plotname_msd = value.replace('.csv', '_MSD.tif') # create a new plotname\n",
    "    os.chdir(FIGmsd_path) # set the working directory to the FIG_MSD folder\n",
    "    plt.savefig(plotname_msd, format = 'tif', dpi = 600) # save the MSD plot \n",
    "    ## calculate the translational component for filtering purposes\n",
    "    msdt = msd.transpose() # transposed the DataFrame to calculate the translational component\n",
    "    msdt.drop(msdt.loc[msdt[1.0]== 0].index, inplace=True)\n",
    "    msdt['tc'] = ((np.log10(msdt[l_t/fr_ps]) - np.log10(d_kt[1/fr_ps]))/(np.log10(l_t/fr_ps)-np.log10(1/fr_ps))) # calculate the translational component based on the MSD curves\n",
    "    os.chdir(DATR_MSD_TC_path) # set the working directory to the DATR_MSD_TC folder\n",
    "    filename_tc = value.replace('.csv', '_tc.csv') # create a new filename\n",
    "    pd.DataFrame(msdt).to_csv(filename_tc) # save the msdt DataFrame including tc data as a .csv file\n",
    "    ## plot a histogram of translational component\n",
    "    plt.figure(figsize = (5,5)) # create an empty plot\n",
    "    ax_tc = plt.subplot(1,1,1) # add axes to the plot\n",
    "    ax_tc.set_xlim([0,2]) # set the x-axis limit\n",
    "    plt.xticks(np.arange(0, 2.2, 0.2)) # set the x-axis ticks\n",
    "    plt.hist(msdt['tc'], bins = np.arange(0,2.1, 0.1)) # plot a histogram of tc data\n",
    "    plotname = Value.replace('.csv', '_tc.tif') # create a new plotname\n",
    "    os.chdir(FIGtc_path) # set the working directory to the FIG_TC folder\n",
    "    plt.savefig(plotname, format = 'tif', dpi = 600) # save the tc plot \n",
    "    print(i, 'out of', len(fl_DATA), ' files have been analyzed for msd and tc')\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Filter out trajectories that are subdiffusive based on their translational component</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DATR_MSD_TC_path) # set the working directory to the DATR_MSD_KT folder\n",
    "fl_DATR_MSD_TC = os.listdir(DATR_MSD_TC_path) # create a list of all filenames\n",
    "co_tc = 1.2 # cutoff value for tc\n",
    "i = 1 # step variable\n",
    "for Value in fl_DATR_MSD_TC:  # for loop that will cycle through each .csv-file\n",
    "    os.chdir(SCALE_path) # set the working directory to the DATR_MSD_KT folder\n",
    "    filename_frames = os.path.splitext(value[0:4])[0] + \"_scaled.tif\"\n",
    "    frames = np.array(imread(filename_frames)) # import the frames as a numpy array\n",
    "    xlim = int(frames.shape[2]) # retrieve the width of the videofile for plotting purposes\n",
    "    ylim = int(frames.shape[1]) # retrieve the height of the videofile for plotting purposes\n",
    "    ## create a list of particles with their unique ID that have a translational component larger or equal than 1.2\n",
    "    os.chdir(DATR_MSD_TC_path) # set the working directory to the DATR_MSD_TC folder\n",
    "    msdt_tc = pd.read_csv(Value, index_col = 0) # import the .csv file as a DataFrame\n",
    "    msdt_tc = msdt_tc.loc[msdt_tc['tc'] >= co_tc] # filter out particles that are not-motile/subdiffusive\n",
    "    dpart =  list(msdt_tc.index.values) # make a list of all particles that are considered to be motile\n",
    "    filename_fe = Value.replace('_tc.csv', '.csv') # create a filename to call on a previously made trajectory .csv file\n",
    "    os.chdir(DATR_f_path) # set the working directory to the DATR_f folder\n",
    "    trf = pd.read_csv(filename_fe) # import the .csv file as a DataFrame\n",
    "    ## filter out all bacteria that did not meet the cut-off value\n",
    "    trf = trf.loc[trf['particle'].isin(dpart)] #filter out particles that have a tc value lower or equal than 1.2\n",
    "    os.chdir(DATR_MSD_f_path) # set the working directory to the DATR_MSD_f folder\n",
    "    filename_trf = filename_fe.replace('.csv', '_MSDfilter.csv') # create a new filename\n",
    "    pd.DataFrame(trf).to_csv(filename_trf) # save the filtered trajectory Dataframe to a .csv file\n",
    "    ## plot the filtered trajectories\n",
    "    plt.figure(figsize = (5,5)) # create an empty plot\n",
    "    ax1 = plt.subplot(1,1,1) # add axes to the plot\n",
    "    ax1.set_xlim([0,x_lim]) # set the x-axis limit\n",
    "    ax1.set_ylim([y_lim,0]) # set the y-axis limit\n",
    "    plt.imshow(frames[int(len(frames)/2)], cmap = 'Greys_r' , vmin = 0, vmax = 255) # add the middle frame as a background image \n",
    "    tp.plot_traj(trf, ax=ax1,colorby = 'particle') # plot the trajectories\n",
    "    plotname = Value.replace('.csv', '_MSD_traj.tif') # create a new plotname\n",
    "    os.chdir(FIG_MSD_traj_path) # set the working directory to the FIG_MSD_traj folder\n",
    "    plt.savefig(plotname, format = 'tif', dpi = 600) # save the plot as a .tif file\n",
    "    print(i, 'out of', len(fl_DATR_MSD_TC), ' files have been msd filtered for non-motile bacteria')\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Count Listeria cells entering and leaving the interaction zone</h4>\n",
    "\n",
    "This script block allowed us to calculate the number of captured Listeria cells per Amoeba and per time point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DATR_MSD_f_path) # set the working directory to the DATR_MSD_f folder\n",
    "fl_DATR_MSD_f = os.listdir(DATR_MSD_f_path) # create a list of all filenames\n",
    "counts = pd.DataFrame(columns = ['time','amoeba_label', 'in', 'out', 'cap']) # create an empty DataFrame with the appropriate headers\n",
    "i = 1 # step variable\n",
    "for Value in fl_DATR_MSD_f: # for loop that will cycle through each .csv-file\n",
    "    os.chdir(DATR_MSD_f_path) # set the working directory to the DATR_MSD_f folder\n",
    "    trf = pd.read_csv(value, index_col = 0) # import the .csv file as a DataFrame\n",
    "    mask = [] # create an empty list\n",
    "    # Analysis with the NORMAL interaction mask\n",
    "    os.chdir(CANNY_INTERACTION_path) # set the working directory to the CANNY_INTERACTION folder\n",
    "    filename_interaction = os.path.splitext(value[0:4])[0] + \"_interaction.tif\" # create a filename to call on the correct interaction mask file\n",
    "    # Analysis with the SMALL interaction mask (uncomment the next two rows if )\n",
    "    #os.chdir(CANNY_INTERACTION_small_path) # set the working directory to the CANNY_INTERACTION_small folder\n",
    "    #filename_interaction = os.path.splitext(value[0:4])[0] + \"_interaction_small.tif\" # create a filename to call on the correct small interaction mask file\n",
    "    frames_interaction = np.array(imread(filename_interaction))  # import the interaction mask file as a numpy array\n",
    "    for f in d_trf['frame.1'].unique(): # for loop that cycles through each frame\n",
    "        frame_data = trf.loc[trf['frame.1'] == f] # select feature data for given frame f\n",
    "        frame_interaction = frames_interaction[f,::] # select frame mask f\n",
    "        frame_index = list(frame_data[[\"y\", \"x\"]].round().astype(int).itertuples(index=False, name=None)) # get list of coordinates of features within frame (rounded upwards and transformed to integer for use in numpy)\n",
    "        frame_mask = np.array([frame_interaction[b] for b in frame_index]) # returns either 0 (outside) or 255 (within) amoeba area\n",
    "        mask.extend(frame_mask) # append into mask list\n",
    "    mask = np.array(mask) # convert from list to numpy array\n",
    "    mask[mask == 255] = 1 # change all 255 cell values to 1\n",
    "    trf['mask'] = mask # add a collumn with the mask values to the trajectories DataFrame\n",
    "    difference = [] # create an ampty list\n",
    "    for p in trf.particle.unique(): # for loop that cycles through each unique particle ID\n",
    "        particle_data = trf.loc[trf['particle'] == p] # select all data corresponding to the respective particle ID\n",
    "        end = (len(particle_data['mask']) - 1) # calculate the cell value with the last mask value\n",
    "        mask_0end = np.array(particle_d['mask'].iloc[0:end], dtype = int) # create a numpy array with all mask values from position 0 to value end\n",
    "        mask_1end = np.array(particle_d['mask'].iloc[1:], dtype = int) # create a numpy array with all mask values from position 1 to value end\n",
    "        sub_difference = np.array(mask_0end - mask_1end) # by taking the difference from the two previous numpy array we know when a cell entered the interaction mask (= -1) or exited the interaction mask (= 1)\n",
    "        difference.extend(sub_difference) # append the difference values into a list\n",
    "    difference = np.array(difference) # convert from list to numpy array\n",
    "    timepoint = int(Value[0:2]) # collect the time point from the file ame\n",
    "    amoeba_label = Value[3] # collect the amoeba-label from the filename\n",
    "    ins = np.count_nonzero(difference == -1) # count the amount of Listeria that entered the interaction mask\n",
    "    outs = np.count_nonzero(difference == 1) # count the amount of Listeria that exited the interaction mask\n",
    "    cap = ins - outs # calculate the amount of captured Listeria cells\n",
    "    values = [(timepoint, amoeba_label, ins, outs, cap)] # make list of all values\n",
    "    sub_counts = pd.DataFrame(values ,columns = ['time','amoeba_label', 'in', 'out', 'cap']) # creata a DataFrame row with all values\n",
    "    counts = counts.append(sub_counts, sort = False) # collect all rows into one DataFrame\n",
    "    print(i,'out of', len(fl_DATR_MSD_f), ' files have been analyzed for capture counts')\n",
    "    i += 1;\n",
    "os.chdir(OUTPUT_path)  # set the working directory to the OUTPUT folder\n",
    "encounter.to_csv('counts_normal.csv')  # save the counts DataFrame as a .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate the Listeria density around each Acanthamoeba</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 165  files have been analyzed for density parameters\n",
      "2 out of 165  files have been analyzed for density parameters\n",
      "3 out of 165  files have been analyzed for density parameters\n",
      "4 out of 165  files have been analyzed for density parameters\n",
      "5 out of 165  files have been analyzed for density parameters\n",
      "6 out of 165  files have been analyzed for density parameters\n",
      "7 out of 165  files have been analyzed for density parameters\n",
      "8 out of 165  files have been analyzed for density parameters\n",
      "9 out of 165  files have been analyzed for density parameters\n",
      "10 out of 165  files have been analyzed for density parameters\n",
      "11 out of 165  files have been analyzed for density parameters\n",
      "12 out of 165  files have been analyzed for density parameters\n",
      "13 out of 165  files have been analyzed for density parameters\n",
      "14 out of 165  files have been analyzed for density parameters\n",
      "15 out of 165  files have been analyzed for density parameters\n",
      "16 out of 165  files have been analyzed for density parameters\n",
      "17 out of 165  files have been analyzed for density parameters\n",
      "18 out of 165  files have been analyzed for density parameters\n",
      "19 out of 165  files have been analyzed for density parameters\n",
      "20 out of 165  files have been analyzed for density parameters\n",
      "21 out of 165  files have been analyzed for density parameters\n",
      "22 out of 165  files have been analyzed for density parameters\n",
      "23 out of 165  files have been analyzed for density parameters\n",
      "24 out of 165  files have been analyzed for density parameters\n",
      "25 out of 165  files have been analyzed for density parameters\n",
      "26 out of 165  files have been analyzed for density parameters\n",
      "27 out of 165  files have been analyzed for density parameters\n",
      "28 out of 165  files have been analyzed for density parameters\n",
      "29 out of 165  files have been analyzed for density parameters\n",
      "30 out of 165  files have been analyzed for density parameters\n",
      "31 out of 165  files have been analyzed for density parameters\n",
      "32 out of 165  files have been analyzed for density parameters\n",
      "33 out of 165  files have been analyzed for density parameters\n",
      "34 out of 165  files have been analyzed for density parameters\n",
      "35 out of 165  files have been analyzed for density parameters\n",
      "36 out of 165  files have been analyzed for density parameters\n",
      "37 out of 165  files have been analyzed for density parameters\n",
      "38 out of 165  files have been analyzed for density parameters\n",
      "39 out of 165  files have been analyzed for density parameters\n",
      "40 out of 165  files have been analyzed for density parameters\n",
      "41 out of 165  files have been analyzed for density parameters\n",
      "42 out of 165  files have been analyzed for density parameters\n",
      "43 out of 165  files have been analyzed for density parameters\n",
      "44 out of 165  files have been analyzed for density parameters\n",
      "45 out of 165  files have been analyzed for density parameters\n",
      "46 out of 165  files have been analyzed for density parameters\n",
      "47 out of 165  files have been analyzed for density parameters\n",
      "48 out of 165  files have been analyzed for density parameters\n",
      "49 out of 165  files have been analyzed for density parameters\n",
      "50 out of 165  files have been analyzed for density parameters\n",
      "51 out of 165  files have been analyzed for density parameters\n",
      "52 out of 165  files have been analyzed for density parameters\n",
      "53 out of 165  files have been analyzed for density parameters\n",
      "54 out of 165  files have been analyzed for density parameters\n",
      "55 out of 165  files have been analyzed for density parameters\n",
      "56 out of 165  files have been analyzed for density parameters\n",
      "57 out of 165  files have been analyzed for density parameters\n",
      "58 out of 165  files have been analyzed for density parameters\n",
      "59 out of 165  files have been analyzed for density parameters\n",
      "60 out of 165  files have been analyzed for density parameters\n",
      "61 out of 165  files have been analyzed for density parameters\n",
      "62 out of 165  files have been analyzed for density parameters\n",
      "63 out of 165  files have been analyzed for density parameters\n",
      "64 out of 165  files have been analyzed for density parameters\n",
      "65 out of 165  files have been analyzed for density parameters\n",
      "66 out of 165  files have been analyzed for density parameters\n",
      "67 out of 165  files have been analyzed for density parameters\n",
      "68 out of 165  files have been analyzed for density parameters\n",
      "69 out of 165  files have been analyzed for density parameters\n",
      "70 out of 165  files have been analyzed for density parameters\n",
      "71 out of 165  files have been analyzed for density parameters\n",
      "72 out of 165  files have been analyzed for density parameters\n",
      "73 out of 165  files have been analyzed for density parameters\n",
      "74 out of 165  files have been analyzed for density parameters\n",
      "75 out of 165  files have been analyzed for density parameters\n",
      "76 out of 165  files have been analyzed for density parameters\n",
      "77 out of 165  files have been analyzed for density parameters\n",
      "78 out of 165  files have been analyzed for density parameters\n",
      "79 out of 165  files have been analyzed for density parameters\n",
      "80 out of 165  files have been analyzed for density parameters\n",
      "81 out of 165  files have been analyzed for density parameters\n",
      "82 out of 165  files have been analyzed for density parameters\n",
      "83 out of 165  files have been analyzed for density parameters\n",
      "84 out of 165  files have been analyzed for density parameters\n",
      "85 out of 165  files have been analyzed for density parameters\n",
      "86 out of 165  files have been analyzed for density parameters\n",
      "87 out of 165  files have been analyzed for density parameters\n",
      "88 out of 165  files have been analyzed for density parameters\n",
      "89 out of 165  files have been analyzed for density parameters\n",
      "90 out of 165  files have been analyzed for density parameters\n",
      "91 out of 165  files have been analyzed for density parameters\n",
      "92 out of 165  files have been analyzed for density parameters\n",
      "93 out of 165  files have been analyzed for density parameters\n",
      "94 out of 165  files have been analyzed for density parameters\n",
      "95 out of 165  files have been analyzed for density parameters\n",
      "96 out of 165  files have been analyzed for density parameters\n",
      "97 out of 165  files have been analyzed for density parameters\n",
      "98 out of 165  files have been analyzed for density parameters\n",
      "99 out of 165  files have been analyzed for density parameters\n",
      "100 out of 165  files have been analyzed for density parameters\n",
      "101 out of 165  files have been analyzed for density parameters\n",
      "102 out of 165  files have been analyzed for density parameters\n",
      "103 out of 165  files have been analyzed for density parameters\n",
      "104 out of 165  files have been analyzed for density parameters\n",
      "105 out of 165  files have been analyzed for density parameters\n",
      "106 out of 165  files have been analyzed for density parameters\n",
      "107 out of 165  files have been analyzed for density parameters\n",
      "108 out of 165  files have been analyzed for density parameters\n",
      "109 out of 165  files have been analyzed for density parameters\n",
      "110 out of 165  files have been analyzed for density parameters\n",
      "111 out of 165  files have been analyzed for density parameters\n",
      "112 out of 165  files have been analyzed for density parameters\n",
      "113 out of 165  files have been analyzed for density parameters\n",
      "114 out of 165  files have been analyzed for density parameters\n",
      "115 out of 165  files have been analyzed for density parameters\n",
      "116 out of 165  files have been analyzed for density parameters\n",
      "117 out of 165  files have been analyzed for density parameters\n",
      "118 out of 165  files have been analyzed for density parameters\n",
      "119 out of 165  files have been analyzed for density parameters\n",
      "120 out of 165  files have been analyzed for density parameters\n",
      "121 out of 165  files have been analyzed for density parameters\n",
      "122 out of 165  files have been analyzed for density parameters\n",
      "123 out of 165  files have been analyzed for density parameters\n",
      "124 out of 165  files have been analyzed for density parameters\n",
      "125 out of 165  files have been analyzed for density parameters\n",
      "126 out of 165  files have been analyzed for density parameters\n",
      "127 out of 165  files have been analyzed for density parameters\n",
      "128 out of 165  files have been analyzed for density parameters\n",
      "129 out of 165  files have been analyzed for density parameters\n",
      "130 out of 165  files have been analyzed for density parameters\n",
      "131 out of 165  files have been analyzed for density parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 out of 165  files have been analyzed for density parameters\n",
      "133 out of 165  files have been analyzed for density parameters\n",
      "134 out of 165  files have been analyzed for density parameters\n",
      "135 out of 165  files have been analyzed for density parameters\n",
      "136 out of 165  files have been analyzed for density parameters\n",
      "137 out of 165  files have been analyzed for density parameters\n",
      "138 out of 165  files have been analyzed for density parameters\n",
      "139 out of 165  files have been analyzed for density parameters\n",
      "140 out of 165  files have been analyzed for density parameters\n",
      "141 out of 165  files have been analyzed for density parameters\n",
      "142 out of 165  files have been analyzed for density parameters\n",
      "143 out of 165  files have been analyzed for density parameters\n",
      "144 out of 165  files have been analyzed for density parameters\n",
      "145 out of 165  files have been analyzed for density parameters\n",
      "146 out of 165  files have been analyzed for density parameters\n",
      "147 out of 165  files have been analyzed for density parameters\n",
      "148 out of 165  files have been analyzed for density parameters\n",
      "149 out of 165  files have been analyzed for density parameters\n",
      "150 out of 165  files have been analyzed for density parameters\n",
      "151 out of 165  files have been analyzed for density parameters\n",
      "152 out of 165  files have been analyzed for density parameters\n",
      "153 out of 165  files have been analyzed for density parameters\n",
      "154 out of 165  files have been analyzed for density parameters\n",
      "155 out of 165  files have been analyzed for density parameters\n",
      "156 out of 165  files have been analyzed for density parameters\n",
      "157 out of 165  files have been analyzed for density parameters\n",
      "158 out of 165  files have been analyzed for density parameters\n",
      "159 out of 165  files have been analyzed for density parameters\n",
      "160 out of 165  files have been analyzed for density parameters\n",
      "161 out of 165  files have been analyzed for density parameters\n",
      "162 out of 165  files have been analyzed for density parameters\n",
      "163 out of 165  files have been analyzed for density parameters\n",
      "164 out of 165  files have been analyzed for density parameters\n",
      "165 out of 165  files have been analyzed for density parameters\n"
     ]
    }
   ],
   "source": [
    "os.chdir(DATR_MSD_f_path) # set the working directory to the DATR_MSD_f folder\n",
    "fl_DATR_MSD_f = os.listdir(DATR_MSD_f_path) # create a list of all filenames\n",
    "os.chdir(OUTPUT_path) # set the working directory to the OUTPUT folder\n",
    "amoeba_size = pd.read_csv('amoeba_size.csv') # import the amoeba_size .csv file\n",
    "density = pd.DataFrame(columns = ['time','amoeba_label', 'traj_tot', 'traj', 'traj_std', 'traj_dens', 'traj_std_dens', 'trajf_tot', 'trajf', 'trajf_std', 'trajf_dens', 'trajf_std_dens']) # create an empty DataFrame with the correct collumn headers\n",
    "i = 1 # step variable\n",
    "xylim = 350 # width and height of the videos in pixels\n",
    "pmc = 3.076923 # pixel per micrometer conversion factor (this was determined by the microscope and remained constant for all encounter rate experiments, since they were all performed on one microscope with constant paramters)\n",
    "xylim_um = xylim/pmc # width and height of the videos in micrometers\n",
    "dof = 10 # this is the assumed depth of field (in micrometer) over which we are detecting and tracking listeria cells\n",
    "for value in fl_DATR_MSD_f:  # for loop that will cycle through each .csv-file\n",
    "    timepoint = int(value[0:2]) # collect the time point from the filename\n",
    "    amoeba_label = value[3] # collect the amoeba-label from the filename\n",
    "    amoeba_area = amoeba_size.loc[(amoeba_size['time'] == timepoint) & (amoeba_size['amoeba_label'] == str(amoeba_label)), 'area_amoeba'].values[0] # get the area of the amoeba, based on the amoeba mask, for the corresponding time point and amoeba label\n",
    "    density_volume = dof*((xylim_um*xylim_um) - amoeba_area) # this is the volume component with which the trajectory counts will be divided to get a cells/micrometer^3 Listeria density                                                               \n",
    "    os.chdir(DATR_path) # set the working directory to the DATR folder\n",
    "    filename_traj = os.path.splitext(value[0:4])[0] + \"_traj.csv\" # create the corresponding trajectory .csv filename\n",
    "    tr = pd.read_csv(filename_traj) # import the trajectory .csv file as a DataFrame\n",
    "    traj_tot = tr['particle'].nunique() # retrieve total number of unique trajectories based on particle ID for the complete time point\n",
    "    traj = np.mean(tr['frame'].value_counts()) # calculate the average number of trajectories per frame\n",
    "    traj_std = np.std(tr['frame'].value_counts()) # calculate the standard deviation of the number of trajectories per frame\n",
    "    traj_dens = traj/density_volume # transform the average number of trajectories per frame into a density\n",
    "    traj_std_dens = traj_std/density_volume # transform the standard deviation of the number of trajectories per frame into a density                                                                   \n",
    "    os.chdir(DATR_MSD_f_path) # set the working directory to the DATR_MSD_f folder\n",
    "    trf = pd.read_csv(value) # import the MSD filtered trajectory .csv file as a DataFrame\n",
    "    trajf_tot = trf['particle'].nunique() # retrieve total number of unique MSD filtered trajectories based on particle ID for the complete time point\n",
    "    trajf = np.mean(trf['frame'].value_counts()) # calculate the average number of MSD filtered trajectories per frame\n",
    "    trajf_std = np.std(trf['frame'].value_counts())  # calculate the standard deviation of the number of MSD filtered trajectories per frame\n",
    "    trajf_dens = trajf/density_volume # transform the average number of MSD filtered trajectories per frame into a density\n",
    "    trajf_std_dens = trajf_std/density_volume # transform the standard deviation of the number of MSD filtered trajectories per frame into a density                                                                   \n",
    "    values = [(timepoint, amoeba_label, traj_tot, traj, traj_std, traj_dens, traj_std_dens, trajf_tot, trajf, trajf_std, trajf_dens, trajf_std_dens)] # collect all parameters into a list\n",
    "    subdensity = pd.DataFrame(values, columns = ['time','amoeba_label', 'traj_tot', 'traj', 'traj_std', 'traj_dens', 'traj_std_dens', 'trajf_tot', 'trajf', 'trajf_std', 'trajf_dens', 'trajf_std_dens']) # creata a DataFrame row from all parameters \n",
    "    density = density.append(subdensity, sort = False) # colect all rows into one DataFrame\n",
    "    print(i,'out of', len(fl_DATR_MSD_f), ' files have been analyzed for density parameters')\n",
    "    i += 1;\n",
    "os.chdir(OUTPUT_path) # set the working directory to the OUTPUT folder\n",
    "density.to_csv('density.csv') # save the density DataFrame as a .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate Listeria swimming speeds</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "2 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "3 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "4 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "5 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "6 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "7 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "8 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "9 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "10 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "11 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "12 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "13 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "14 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "15 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "16 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "17 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "18 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "19 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "20 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "21 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "22 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "23 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "24 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "25 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "26 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "27 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "28 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "29 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "30 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "31 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "32 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "33 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "34 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "35 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "36 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "37 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "38 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "39 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "40 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "41 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "42 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "43 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "44 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "45 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "46 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "47 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "48 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "49 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "50 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "51 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "52 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "53 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "54 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "55 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "56 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "57 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "58 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "59 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "60 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "61 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "62 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "63 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "64 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "65 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "66 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "67 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "68 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "69 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "70 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "71 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "72 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "73 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "74 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "75 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "76 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "77 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "78 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "79 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "80 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "81 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "82 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "83 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "84 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "85 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "86 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "87 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "88 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "89 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "90 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "91 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "92 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "93 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "94 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "95 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "96 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "97 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "98 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "99 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "100 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "101 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "102 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "103 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "104 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "105 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "106 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "107 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "108 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "109 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "110 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "111 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "112 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "113 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "114 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "115 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "116 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "117 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "118 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "119 out of 165  files have been analyzed for Listeria swimming speeds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "121 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "122 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "123 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "124 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "125 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "126 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "127 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "128 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "129 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "130 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "131 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "132 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "133 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "134 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "135 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "136 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "137 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "138 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "139 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "140 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "141 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "142 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "143 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "144 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "145 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "146 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "147 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "148 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "149 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "150 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "151 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "152 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "153 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "154 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "155 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "156 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "157 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "158 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "159 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "160 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "161 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "162 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "163 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "164 out of 165  files have been analyzed for Listeria swimming speeds\n",
      "165 out of 165  files have been analyzed for Listeria swimming speeds\n"
     ]
    }
   ],
   "source": [
    "os.chdir(DATR_MSD_f_path) # set the working directory to the DATR_MSD_f folder\n",
    "FL_DATR_MSD_f = os.listdir(DATR_MSD_f_path) # create a list of all filenames\n",
    "te = 120 # total time of the video in seconds\n",
    "pmc = 3.076923 # pixel per micrometer conversion factor (this was determined by the microscope and remained constant for all encounter rate experiments, since they were all performed on one microscope with constant paramters)\n",
    "i = 1 # step variable\n",
    "v_tot = pd.DataFrame() # creata an empty DataFrame\n",
    "for Value in FL_DATR_MSD_f: # for loop that will cycle through each .csv-file\n",
    "    os.chdir(DATR_MSD_f_path) # set the working directory to the DATR_MSD_f folder\n",
    "    trf = pd.read_csv(Value) # import the MSD filtered trajectory .csv file as a DataFrame\n",
    "    tot_frame = trf['frame'].max()+1 # determine the total amount of frames, the +1 modifier is to account for the initial frame value of 0\n",
    "    fps = round((tot_frame + 1)/te) # average frames per second based on the total frames divided by the total time of each video\n",
    "    part_id = pd.DataFrame(trf.particle.unique(), columns = ['pID']) # creata a list of all unique particle Id's\n",
    "    #mp = part_id.pID.nunique()\n",
    "    d = pd.DataFrame() # creata an empty DataFrame\n",
    "    v_data = pd.DataFrame(columns = ['time', 'amoeba_label', 'v_mean', 'v_std', 'pID']) # create an empty DataFrame\n",
    "    timepoint = int(Value[0:2]) # collect the time point from the filename\n",
    "    amoeba_label = Value[3] # collect the amoeba-label from the filename\n",
    "    for p in set(trf.particle): # for loop that cycles through each unique particle ID\n",
    "        p_data = trf[trf.particle == p] # select all data from the MSD filtered trajectory DataFrame for a given particle ID\n",
    "        dx = np.diff(p_data.x)/pmc # calculate the difference between consecutive x-coordinates\n",
    "        dy = np.diff(p_data.y)/pmc # calculate the difference between consecutive y-coordinates\n",
    "        #if fps == 0: # if else function in case \n",
    "        #    v = 0\n",
    "        #else:\n",
    "        v = (np.sqrt(dx**2+dy**2)/np.diff(p_data.frame))*fps # calculate the swimming speed between each recorded coordinate by caulculated the pythagorean distance divided by the number of frames between each consecutive coordinated, multiplied by the frames per second \n",
    "        v_mean = np.mean(v) # calculate the mean swimming speed\n",
    "        v_std = np.std(v)\n",
    "        values = [(timepoint, amoeba_label, v_mean, v_std, p)] # create a list of all calculated parameters\n",
    "        sub_v_data = pd.DataFrame(values, columns = ['time', 'amoeba_label', 'v_mean', 'v_std', 'pID']) # creata a DataFrame row from all parameters \n",
    "        v_data = v_data.append(sub_v_data, sort = False) # collect all rows into one DataFrame\n",
    "    # save this dataframe as a csv\n",
    "    print(i, 'out of', len(FL_DATR_MSD_f), ' files have been analyzed for Listeria swimming speeds')\n",
    "    i += 1\n",
    "    v_tot = pd.concat((v_tot, v_data), axis=0) # combine all DataFrames into one\n",
    "# Average swimming speeds for each Amoeba or for each timepoint\n",
    "v_amoeba = pd.DataFrame() # creata an empty DataFrame\n",
    "v_time = pd. DataFrame() # creata an empty DataFrame\n",
    "time_list = list(np.unique(v_tot.time)) # create a list of each timepoint\n",
    "for t in time_list: # for loop that will cycle through each timepoint\n",
    "    v_tot_t = v_tot[v_tot.time == t] # select all data from the swimming speed DataFrame for a given timepoint\n",
    "    v_mean_t = np.mean(v_tot_t.v_mean) # calculate the mean swimming speed\n",
    "    v_std_t = np.std(v_tot_t.v_mean) # calculate the standard deviation of the swimming speed\n",
    "    values_t = [(t , v_mean_t, v_std_t)] # create a list of all calculated parameters\n",
    "    v_time_sub = pd.DataFrame(values_t, columns = ['time', 'v_mean', 'v_std'])  # creata a DataFrame row from all parameters \n",
    "    v_time = v_time.append(v_time_sub, sort = False) # collect all rows into one DataFrame\n",
    "    amoeba_list = list(np.unique(v_tot_t.amoeba_label)) # create a list off all amoeba labels\n",
    "    for a in amoeba_list: # for loop that will cycle through each amoeba label\n",
    "        v_tot_t_a = v_tot_t[v_tot_t.amoeba_label == a] # select all data from the swimming speed for each timepoint DataFrame for a given timepoint\n",
    "        v_mean_a = np.mean(v_tot_t_a.v_mean) # calculate the mean swimming speed\n",
    "        v_std_a = np.std(v_tot_t_a.v_mean) # calculate the standard deviation of the swimming speed\n",
    "        values_a = [(t , a, v_mean_a, v_std_a)] # create a list of all calculated parameters\n",
    "        sub_v_amoeba = pd.DataFrame(values_a, columns = ['time', 'amoeba_label', 'v_mean', 'v_std'])  # creata a DataFrame row from all parameters \n",
    "        v_amoeba = v_amoeba.append(sub_v_amoeba, sort = False) # collect all rows into one DataFrame\n",
    "os.chdir(OUTPUT_path) # set the working directory to the OUTPUT folder\n",
    "pd.DataFrame(v_tot).to_csv('swimmingspeed_pt_pa_piD.csv') # save the mean swimming speed per time point, amoeba and particle ID Dataframe as a .csv file\n",
    "v_amoeba.to_csv('swimmingspeed__pt_pa.csv')   # save the mean swimming speed per timepoint and amoeba Dataframe as a .csv file   \n",
    "v_time.to_csv('swimmingspeed_pt.csv')  # save the mean swimming speed per time point Dataframe as a .csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(CANNY_INTERACTION_path)\n",
    "fl_list = os.listdir(CANNY_INTERACTION_path)\n",
    "# Absolute path of a file\n",
    "for value in fl_list:\n",
    "    old_name = value\n",
    "    new_name = value.replace('_dilation.tif', '_interaction.tif')\n",
    "    os.rename(old_name, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(CANNY_INTERACTION_small_path)\n",
    "fl_list = os.listdir(CANNY_INTERACTION_small_path)\n",
    "# Absolute path of a file\n",
    "for value in fl_list:\n",
    "    old_name = value\n",
    "    new_name = value.replace('_dilationsmall.tif', '_interaction_small.tif')\n",
    "    os.rename(old_name, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(CANNY_AMOEBA_path)\n",
    "fl_list = os.listdir(CANNY_AMOEBA_path)\n",
    "# Absolute path of a file\n",
    "for value in fl_list:\n",
    "    old_name = value\n",
    "    new_name = value.replace('_erosion.tif', '_amoeba.tif')\n",
    "    os.rename(old_name, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DATR_MSD_TC_path)\n",
    "fl_list = os.listdir(DATR_MSD_TC_path)\n",
    "# Absolute path of a file\n",
    "for value in fl_list:\n",
    "    old_name = value\n",
    "    new_name = value.replace('_kt.csv', '_tc.csv')\n",
    "    os.rename(old_name, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(FIG_TC_path)\n",
    "fl_list = os.listdir(FIG_TC_path)\n",
    "# Absolute path of a file\n",
    "for value in fl_list:\n",
    "    old_name = value\n",
    "    new_name = value.replace('_kt.tif', '_tc.tif')\n",
    "    os.rename(old_name, new_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
